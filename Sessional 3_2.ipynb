{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad8bfdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652f4dc",
   "metadata": {},
   "source": [
    "#### 1. Create an inverted index for the corpus. The index should contain only the document postings. What is the time taken to create this index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "703bdd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CORPUS = \"F:\\\\IR Lab\\\\bbc\\\\\"\n",
    "stopwords = open(\"F:\\\\IR Lab\\\\stopwords.txt\",\"r\", encoding = \"utf-8\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af4825e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeWithOffset(file_text, limit = 3):\n",
    "    stopwords = open(\"F:\\\\IR Lab\\\\stopwords.txt\",\"r\", encoding = \"utf-8\").read().split(\"\\n\")\n",
    "    text = re.sub(r'\\s+\\n', ' ', file_text)\n",
    "    pattern = re.compile(r\"^\\w*[-']?\\w*$\")\n",
    "    better_tokens = list(filter(pattern.match, text.split(\" \")))\n",
    "    token_list = [ele.lower() for ele in better_tokens if len(ele) > limit and ele.lower() not in stopwords]\n",
    "    \n",
    "    extendedToken_list = {}\n",
    "    for i, token in enumerate(token_list):\n",
    "        if extendedToken_list.get(token) != None:\n",
    "            extendedToken_list[token].append(i)\n",
    "        else:\n",
    "            extendedToken_list[token] = [i]\n",
    "    \n",
    "    return extendedToken_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5d65eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file_text, limit = 3):\n",
    "    stopwords = open(\"F:\\\\IR Lab\\\\stopwords.txt\",\"r\", encoding = \"utf-8\").read().split(\"\\n\")\n",
    "    text = re.sub(r'\\s+\\n', ' ', file_text)\n",
    "    pattern = re.compile(r\"^\\w*[-']?\\w*$\")\n",
    "    better_tokens = list(filter(pattern.match, text.split(\" \")))\n",
    "    token_list = [ele.lower() for ele in better_tokens if len(ele) > limit and ele.lower() not in stopwords]\n",
    "    return list(set(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d55a56a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "posting = {}\n",
    "token_per_file = []\n",
    "\n",
    "subfolder_names = os.listdir(PATH_TO_CORPUS)\n",
    "ind = 0;\n",
    "bar = progressbar.ProgressBar(len(subfolder_names) - 1).start()\n",
    "for i, subfolder in enumerate(subfolder_names):\n",
    "    if os.path.isdir(PATH_TO_CORPUS + subfolder):\n",
    "        textfiles = os.listdir(PATH_TO_CORPUS + subfolder +\"\\\\\")\n",
    "        for data in textfiles:\n",
    "            \n",
    "            try:\n",
    "                file_text = open(PATH_TO_CORPUS + subfolder + \"\\\\\" + data,\"r\",encoding=\"utf-8\").read()\n",
    "                ind = ind + 1\n",
    "                tokens = tokenize(file_text)\n",
    "                \n",
    "                token_per_file.append(tokens)\n",
    "                for tk in tokens:\n",
    "                    vocab.append(tk)   \n",
    "                    if posting.get(tk) == None:            \n",
    "                        posting[tk] = [ind]\n",
    "                    else:\n",
    "                        posting[tk].append(ind)\n",
    "            except:\n",
    "                ind = ind - 1\n",
    "                \n",
    "    bar.update(i)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cbc52c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28435"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.sort()\n",
    "vocab = list(set(vocab))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61a165de",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_list = OrderedDict(sorted(posting.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7036f8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[670,\n",
       " 1337,\n",
       " 1831,\n",
       " 1862,\n",
       " 1866,\n",
       " 1886,\n",
       " 1887,\n",
       " 1890,\n",
       " 1908,\n",
       " 1915,\n",
       " 1930,\n",
       " 1932,\n",
       " 1949,\n",
       " 1950,\n",
       " 1970,\n",
       " 1994,\n",
       " 1996,\n",
       " 2035,\n",
       " 2051,\n",
       " 2059,\n",
       " 2070,\n",
       " 2081,\n",
       " 2082,\n",
       " 2101,\n",
       " 2106,\n",
       " 2107,\n",
       " 2108,\n",
       " 2119,\n",
       " 2122,\n",
       " 2123,\n",
       " 2124,\n",
       " 2126,\n",
       " 2129,\n",
       " 2146,\n",
       " 2154,\n",
       " 2158,\n",
       " 2160,\n",
       " 2169,\n",
       " 2177,\n",
       " 2180,\n",
       " 2186,\n",
       " 2196,\n",
       " 2201,\n",
       " 2213]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posting_list[\"apple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6010cbb",
   "metadata": {},
   "source": [
    "#### 2.  Write a function to query against the index for a single token. On an average what is the query time for a single word query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2e9a00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : agency\n",
      "time :  14.290163278579712  second\n",
      "Result :  [28, 35, 39, 48, 91, 131, 162, 169, 185, 191, 203, 222, 236, 264, 275, 282, 313, 318, 334, 338, 345, 353, 363, 368, 388, 389, 437, 445, 454, 460, 472, 475, 491, 497, 537, 685, 805, 905, 972, 983, 986, 1012, 1023, 1046, 1052, 1100, 1211, 1339, 1348, 1349, 1352, 1358, 1362, 1392, 1401, 1404, 1759, 1824, 1861, 1874, 1894, 1927, 1954, 1998, 2028, 2045, 2093, 2156, 2185]\n"
     ]
    }
   ],
   "source": [
    "def query_posting(query):\n",
    "    return posting_list[query]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "query = input(\"Enter Query : \")\n",
    "if posting_list.get(query):\n",
    "    \n",
    "    result = query_posting(query)\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"time : \", (end - start), \" second\")\n",
    "    print(\"Result : \", result)\n",
    "else:\n",
    "    print(\"Query not Present\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa521f6",
   "metadata": {},
   "source": [
    "#### 3. Extend the index created in the first question to contain the offset of the terms in the vocabulary in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7433b7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    }
   ],
   "source": [
    "ExtendedPosting = {}\n",
    "# token_per_file = []\n",
    "\n",
    "subfolder_names = os.listdir(PATH_TO_CORPUS)\n",
    "ind = 0;\n",
    "bar = progressbar.ProgressBar(len(subfolder_names) - 1).start()\n",
    "for i, subfolder in enumerate(subfolder_names):\n",
    "    if os.path.isdir(PATH_TO_CORPUS + subfolder):\n",
    "        textfiles = os.listdir(PATH_TO_CORPUS + subfolder +\"\\\\\")\n",
    "        for data in textfiles:\n",
    "            \n",
    "            try:\n",
    "                file_text = open(PATH_TO_CORPUS + subfolder + \"\\\\\" + data,\"r\",encoding=\"utf-8\").read()\n",
    "                ind = ind + 1\n",
    "                tokens = tokenizeWithOffset(file_text)\n",
    "#                 token_per_file.append(tokens)\n",
    "                \n",
    "                for tk in tokens.keys(): \n",
    "                    if ExtendedPosting.get(tk) == None:            \n",
    "                        ExtendedPosting[tk] = [[ind, tokens[tk]]]\n",
    "                    else:\n",
    "                        ExtendedPosting[tk].append([ind, tokens[tk]])\n",
    "            except:\n",
    "                ind = ind - 1\n",
    "                \n",
    "    bar.update(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "764fd291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_query(p1,p2,k):\n",
    "    res = []\n",
    "    pt1 = 0\n",
    "    pt2 = 0\n",
    "    \n",
    "    while(pt1 < len(p1) and pt2 < len(p2)):\n",
    "        \n",
    "        if p1[pt1][0]  == p2[pt2][0]:\n",
    "            pp1 = p1[pt1][1]\n",
    "            pp2 = p2[pt2][1]\n",
    "            for posp1 in pp1:\n",
    "                l = []\n",
    "                for posp2 in pp2:\n",
    "                    if abs(posp1 - posp2) <= k:\n",
    "                        l.append(posp2)\n",
    "                    elif posp2 - posp1 > k:\n",
    "                        break;\n",
    "                if len(l) > 0:\n",
    "                    res.append([pt1, [posp1, [i for i in l]]])\n",
    "            pt1 = pt1 + 1\n",
    "            pt2 = pt2 + 1\n",
    "            \n",
    "        elif p1[pt1][0]  < p2[pt2][0]:\n",
    "            pt1 = pt1 + 1\n",
    "            \n",
    "        else:\n",
    "            pt2 = pt2 + 1\n",
    "            \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "95472974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : previous attempt\n",
      "[[88, [57, [58]]]]\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query : \")\n",
    "qterm = query.split(\" \")\n",
    "\n",
    "if ExtendedPosting.get(qterm[0]) != None and ExtendedPosting[qterm[1]] != None:\n",
    "    print(positional_query(ExtendedPosting[qterm[0]],ExtendedPosting[qterm[1]], 1))\n",
    "else:\n",
    "    print(\"Query Term not present in vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a2a14ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersectPhrase(query):\n",
    "    qterm = [i for i in query.split(\" \") if i not in stopwords]\n",
    "    print(qterm)\n",
    "    conj = []\n",
    "    for i in range(0,len(qterm)-1):\n",
    "        conj.append(positional_query(ExtendedPosting[qterm[i]],ExtendedPosting[qterm[i+1]],1))\n",
    "    \n",
    "    print(conj)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f14a3088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['previous', 'attempt', 'sell']\n",
      "[[[88, [57, [58]]]], [[30, [5, [6]]], [30, [58, [59]]], [30, [99, [100]]]]]\n"
     ]
    }
   ],
   "source": [
    "intersectPhrase(\"previous attempt to sell\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
