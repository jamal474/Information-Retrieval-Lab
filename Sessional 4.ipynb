{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "stopwords = open(\"F:\\\\IR Lab\\\\stopwords.txt\",\"r\", encoding = \"utf-8\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, min_length = 3, lower = False):\n",
    "    text = re.sub(r'\\s+',\" \",text)\n",
    "#     spec = re.compile(r\"[^A-Za-z0-9]\")\n",
    "#     temp = re.sub(spec,\" \", text)\n",
    "    pattern = re.compile(r'^\\w*[-\\']?\\w*$')\n",
    "    text_sep = [tk for tk in list(filter(pattern.match,text.split(\" \"))) if len(tk) > min_length and tk not in stopwords]\n",
    "    if lower:\n",
    "        return [tk.lower() for tk in text_sep]\n",
    "        \n",
    "    else:\n",
    "        return text_sep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(posting1 , posting2):\n",
    "    posting_intersect = []\n",
    "    n1 = len(posting1)\n",
    "    n2 = len(posting2)\n",
    "    \n",
    "    pt1 = 0\n",
    "    pt2 = 0\n",
    "    \n",
    "    while(pt1 < n1 and pt2 < n2):\n",
    "        \n",
    "        if posting1[pt1] == posting2[pt2]:\n",
    "            posting_intersect.append(posting1[pt1])\n",
    "            pt1 = pt1 + 1\n",
    "            pt2 = pt2 + 1\n",
    "            \n",
    "        elif posting1[pt1] <posting2[pt2] :\n",
    "            pt1 = pt1 + 1;\n",
    "        \n",
    "        elif posting1[pt1] > posting2[pt2]:\n",
    "            pt2 = pt2 + 1;\n",
    "    \n",
    "    return posting_intersect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_intersect(postings):\n",
    "    postings.sort(key = len)\n",
    "    \n",
    "    post_len = len(postings)\n",
    "    temp = postings[0]\n",
    "    \n",
    "    for i in range(1,post_len):\n",
    "        temp = intersect(temp,postings[i])\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['condition serious']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_phrase_query(query):\n",
    "    tokens = tokenize_text(query,3,True)\n",
    "    bigrams = []\n",
    "    for tk in range(0,len(tokens)-1):\n",
    "        bigrams.append(tokens[tk] + \" \"+   tokens[tk+1])\n",
    "        \n",
    "    return bigrams\n",
    "\n",
    "parse_phrase_query(\"very serious condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_corpus = \"F:\\\\IR Lab\\\\bbcsport\\\\\"\n",
    "pfiles = os.listdir(path_to_corpus)\n",
    "\n",
    "tokens_p_file = []\n",
    "post = {}\n",
    "\n",
    "for pfile in pfiles:\n",
    "    if(os.path.isdir(path_to_corpus + pfile)):\n",
    "        files = os.listdir(path_to_corpus + pfile)\n",
    "        ind = 0\n",
    "        for file in files:\n",
    "            textfile = open(path_to_corpus + pfile + \"\\\\\" + file,'r').read()\n",
    "            unigrams = tokenize_text(textfile,3,True)\n",
    "            bigrams = []\n",
    "            for i in range(0,len(unigrams)-1):\n",
    "                bigrams.append(unigrams[i] + \" \"+   unigrams[i+1])\n",
    "            \n",
    "            unigrams = list(set(unigrams))\n",
    "            for tk in unigrams:\n",
    "                if post.get(tk) == None:\n",
    "                    post[tk] = [ind]\n",
    "                else:\n",
    "                    post[tk].append(ind)\n",
    "            bigrams = list(set(bigrams))\n",
    "            for tk in bigrams:\n",
    "                if post.get(tk) == None:\n",
    "                    post[tk] = [ind]\n",
    "                else:\n",
    "                    post[tk].append(ind)   \n",
    "            ind = ind + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95900"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_query(query):\n",
    "    qterms = [ i.lower() for i in query.split(\" \") if len(i) > 3 and i.lower() not in stopwords]\n",
    "    qpost = []\n",
    "    for i in range(0,len(qterms)-1):\n",
    "        if post.get(qterms[i] + \" \" + qterms[i]) != None :\n",
    "            qpost.append(post[qterms[i] + \" \" + qterms[i]])\n",
    "            \n",
    "    return extended_intersect(qpost)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : seemed unrepentant\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter Query : \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(phrase_query(query))\n",
      "Cell \u001b[1;32mIn[77], line 8\u001b[0m, in \u001b[0;36mphrase_query\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m post\u001b[38;5;241m.\u001b[39mget(qterms[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m qterms[i]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[0;32m      6\u001b[0m         qpost\u001b[38;5;241m.\u001b[39mappend(post[qterms[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m qterms[i]])\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extended_intersect(qpost)\n",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m, in \u001b[0;36mextended_intersect\u001b[1;34m(postings)\u001b[0m\n\u001b[0;32m      2\u001b[0m postings\u001b[38;5;241m.\u001b[39msort(key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m      4\u001b[0m post_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(postings)\n\u001b[1;32m----> 5\u001b[0m temp \u001b[38;5;241m=\u001b[39m postings[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,post_len):\n\u001b[0;32m      8\u001b[0m     temp \u001b[38;5;241m=\u001b[39m intersect(temp,postings[i])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter Query : \")\n",
    "print(phrase_query(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
