{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, min_length = 3, lower = False):\n",
    "    text = re.sub(r'\\s+',\" \",text)\n",
    "    spec = re.compile(r\"[^A-Za-z]\\s\")\n",
    "    pattern = re.compile(r'^[A-Za-z]*[-\\']?[A-Za-z]*$')\n",
    "    temp = re.sub(spec,\"\", text)\n",
    "    text_sep = [tk for tk in list(filter(pattern.match,text.split(\" \"))) if len(tk) > min_length]\n",
    "    if lower:\n",
    "        return list(set([tk.lower() for tk in text_sep]))\n",
    "        \n",
    "    else:\n",
    "        return list(set(text_sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(posting1 , posting2):\n",
    "    posting_intersect = []\n",
    "    n1 = len(posting1)\n",
    "    n2 = len(posting2)\n",
    "\n",
    "    pt1 = 0\n",
    "    pt2 = 0\n",
    "    \n",
    "    while(pt1 < n1 and pt2 < n2):\n",
    "        \n",
    "        if int(posting1[pt1]) == int(posting2[pt2]):\n",
    "            posting_intersect.append(posting1[pt1])\n",
    "            pt1 = pt1 + 1\n",
    "            pt2 = pt2 + 1\n",
    "#             print(\"A\",\"\\n\")\n",
    "            \n",
    "        elif int(posting1[pt1]) < int(posting2[pt2]) :\n",
    "            pt1 = pt1 + 1;\n",
    "#             print(\"B\",\"\\n\")\n",
    "\n",
    "        \n",
    "        elif int(posting1[pt1]) > int(posting2[pt2]):\n",
    "            pt2 = pt2 + 1;\n",
    "#             print(\"C\",\"\\n\")\n",
    "\n",
    "        \n",
    "    return posting_intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_intersect(postings):\n",
    "    postings.sort(key = len)\n",
    "    post_len = len(postings)\n",
    "    temp = postings[0]\n",
    "    \n",
    "    for i in range(1,post_len):\n",
    "        temp = intersect(temp,postings[i])\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_corpus = \"C:\\\\Users\\\\acer\\\\Desktop\\\\md shabbir jamal\\\\bbcsport\\\\\"\n",
    "path_to_posting = \"C:\\\\Users\\\\acer\\\\Desktop\\\\md shabbir jamal\\\\postings\\\\\"\n",
    "sport_cat = os.listdir(path_to_corpus)\n",
    "\n",
    "plist = {}\n",
    "glob_count = 0\n",
    "for sport in sport_cat:\n",
    "    files = os.listdir(path_to_corpus + sport + \"\\\\\")\n",
    "    for file in files:\n",
    "        glob_count  = glob_count + 1\n",
    "        text = open(path_to_corpus + sport + \"\\\\\" + file,'r').read()\n",
    "        unitokens = tokenize_text(text,3,True)\n",
    "\n",
    "        for token in unitokens:\n",
    "            plist[token] = token + '.txt'\n",
    "#             if os.path.isfile(path_to_posting +token + '.txt'):\n",
    "#                 fp = open(path_to_posting +token + '.txt',\"w\")\n",
    "#             else:\n",
    "#                 fp = open(path_to_posting +token + '.txt',\"a\")\n",
    "            fp = open(path_to_posting +token + '.txt',\"a\")\n",
    "            fp.write(str(glob_count) + \",\")\n",
    "            fp.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Australia paceman\"\n",
    "qt = tokenize_text(query,3,True)\n",
    "qdids = []\n",
    "for tk in qt:\n",
    "    if plist.get(tk):\n",
    "        fp = open(path_to_posting +plist[tk],\"r\")\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    pt = fp.read()\n",
    "    qdids.append(pt.split(\",\")[:-1])\n",
    "    \n",
    "ans = extended_intersect(qdids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['108', '109', '113', '145', '146', '163', '171', '175', '182', '183', '184']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = {}\n",
    "\n",
    "gt = 0\n",
    "for sport in sport_cat:\n",
    "    files = os.listdir(path_to_corpus + sport + \"\\\\\")\n",
    "    for file in files:\n",
    "        gt = gt + 1\n",
    "        text = open(path_to_corpus + sport + \"\\\\\" + file,'r').read()\n",
    "        unitokens = tokenize_text(text,3,True)\n",
    "        \n",
    "        for tk in unitokens:\n",
    "            if post.get(tk) == None:\n",
    "                post[tk] = [str(gt)]\n",
    "            else:\n",
    "                post[tk].append(str(gt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['108',\n",
       " '109',\n",
       " '112',\n",
       " '113',\n",
       " '132',\n",
       " '145',\n",
       " '146',\n",
       " '163',\n",
       " '166',\n",
       " '171',\n",
       " '175',\n",
       " '180',\n",
       " '182',\n",
       " '183',\n",
       " '184']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post['paceman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_new_posting = \"C:\\\\Users\\\\acer\\\\Desktop\\\\md shabbir jamal\\\\\"\n",
    "plist2 = {}\n",
    "for sport in sport_cat:\n",
    "    files = os.listdir(path_to_corpus + sport + \"\\\\\")\n",
    "    fp = open(path_to_new_posting + )\n",
    "    for file in files:\n",
    "        text = open(path_to_corpus + sport + \"\\\\\" + file,'r').read()\n",
    "        unitokens = tokenize_text(text,3,True)\n",
    "\n",
    "        for token in unitokens:\n",
    "            plist[token] = token + '.txt'\n",
    "            fp = open(path_to_posting +token + '.txt',\"a\")\n",
    "            fp.write(sport+ \"/\" + file[:-3])\n",
    "            fp.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
